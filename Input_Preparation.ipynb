{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Encoder Input Preparation\n",
    "\n",
    "Prepares inputs for `libtext_encoder_htp.so` on QCS6490.\n",
    "\n",
    "| Input | ONNX Shape | QNN Shape | dtype | Quantization |\n",
    "|-------|------------|-----------|-------|-------------|\n",
    "| text_ids | (1, 128) | (1, 128) | INT32 | None |\n",
    "| style_ttl | (1, 50, 256) | (1, 256, 50) | UINT8 | scale=0.00541, offset=-172 |\n",
    "| text_mask | (1, 1, 128) | (1, 128, 1) | UINT8 | scale=0.00392, offset=0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.makedirs('./inputs', exist_ok=True)\n",
    "\n",
    "def quantize(data, scale, offset):\n",
    "    \"\"\"Quantize float32 to uint8: q = round(x/scale) - offset, clipped to [0,255]\"\"\"\n",
    "    return np.clip(np.round(data / scale) - offset, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_ids: 128 elements, 512 bytes\n"
     ]
    }
   ],
   "source": [
    "# 1. text_ids - INT32, no transpose needed\n",
    "text_ids = np.fromfile('qnn_calibration/text_encoder/text_ids.raw', dtype=np.int32)\n",
    "text_ids.tofile('./inputs/text_ids.raw')\n",
    "print(f'text_ids: {text_ids.size} elements, {os.path.getsize(\"./inputs/text_ids.raw\")} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style_ttl: (1, 256, 50), 12800 bytes\n"
     ]
    }
   ],
   "source": [
    "# 2. style_ttl - transpose (1,50,256) -> (1,256,50), then quantize\n",
    "style_ttl = np.fromfile('qnn_calibration/text_encoder/style_ttl.raw', dtype=np.float32)\n",
    "style_ttl = style_ttl.reshape(1, 50, 256).transpose(0, 2, 1)\n",
    "style_ttl_q = quantize(style_ttl, scale=0.0054120589047670, offset=-172)\n",
    "style_ttl_q.tofile('./inputs/style_ttl.raw')\n",
    "print(f'style_ttl: {style_ttl_q.shape}, {os.path.getsize(\"./inputs/style_ttl.raw\")} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_mask: (1, 128, 1), 128 bytes\n"
     ]
    }
   ],
   "source": [
    "# 3. text_mask - transpose (1,1,128) -> (1,128,1), then quantize\n",
    "text_mask = np.fromfile('qnn_calibration/text_encoder/text_mask.raw', dtype=np.float32)\n",
    "text_mask = text_mask.reshape(1, 1, 128).transpose(0, 2, 1)\n",
    "text_mask_q = quantize(text_mask, scale=0.0039215688593686, offset=0)\n",
    "text_mask_q.tofile('./inputs/text_mask.raw')\n",
    "print(f'text_mask: {text_mask_q.shape}, {os.path.getsize(\"./inputs/text_mask.raw\")} bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "```bash\n",
    "qnn-net-run --model ./libtext_encoder_htp.so \\\n",
    "            --backend libQnnHtp.so \\\n",
    "            --input_list text_enc_input.txt \\\n",
    "            --output_dir text_output \\\n",
    "            --use_native_input_files\n",
    "```\n",
    "\n",
    "**Note:** `--use_native_input_files` is required since inputs are pre-quantized.\n",
    "\n",
    "## Output\n",
    "| Output | QNN Shape | dtype | Size |\n",
    "|--------|-----------|-------|------|\n",
    "| text_emb | (1, 128, 256) | FLOAT32 | 131,072 bytes |\n",
    "\n",
    "**Important:** Output is transposed relative to calibration. To compare:\n",
    "```python\n",
    "output = np.fromfile('text_emb.raw', dtype=np.float32).reshape(1, 128, 256)\n",
    "output_t = output.transpose(0, 2, 1)  # -> (1, 256, 128) to match calibration\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Estimator Input Preparation\n",
    "\n",
    "Prepares inputs for `libvector_estimator_htp.so` on QCS6490.\n",
    "\n",
    "| Input | ONNX Shape | QNN Shape | dtype | Quantization |\n",
    "|-------|------------|-----------|-------|--------------|\n",
    "| noisy_latent | (1, 144, 256) | (1, 256, 144) | UINT8 | scale=0.03227, offset=-127 |\n",
    "| text_emb | (1, 128, 256) {text encoder output} | (1, 128, 256) | UINT8 | scale=0.02788, offset=-128 |\n",
    "| style_ttl | (1, 50, 256) | (1, 256, 50) | UINT8 | scale=0.00541, offset=-172 |\n",
    "| latent_mask | (1, 1, 256) | (1, 256, 1) | UINT8 | scale=0.00392, offset=0 |\n",
    "| text_mask | (1, 1, 128) | (1, 128, 1) | UINT8 | scale=0.00392, offset=0 |\n",
    "| current_step | (1,) | (1,) | UINT8 | scale=3.92e-7, offset=0 |\n",
    "| total_step | (1,) | (1,) | UINT8 | scale=0.01961, offset=0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. noisy_latent - transpose (1,144,256) -> (1,256,144), then quantize\n",
    "noisy_latent = np.fromfile('qnn_calibration/vector_estimator/noisy_latent.raw', dtype=np.float32)\n",
    "noisy_latent = noisy_latent.reshape(1, 144, 256).transpose(0, 2, 1)\n",
    "noisy_latent_q = quantize(noisy_latent, scale=0.0322749949991703, offset=-127)\n",
    "noisy_latent_q.tofile('./inputs/noisy_latent.raw')\n",
    "print(f'noisy_latent: {noisy_latent_q.shape}, {os.path.getsize(\"./inputs/noisy_latent.raw\")} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. text_emb - from text_encoder output (board_output), quantize to UINT8\n",
    "# Note: text_encoder outputs (1,128,256), vector_estimator expects (1,128,256) - no transpose needed\n",
    "text_emb = np.fromfile('./board_output/text_emb.raw', dtype=np.float32)\n",
    "text_emb_q = quantize(text_emb, scale=0.0278750918805599, offset=-128)\n",
    "text_emb_q.tofile('./inputs/text_emb.raw')\n",
    "print(f'text_emb: {text_emb_q.size} elements, {os.path.getsize(\"./inputs/text_emb.raw\")} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. style_ttl - already prepared above (same quantization params)\n",
    "# 4. latent_mask - transpose (1,1,256) -> (1,256,1), then quantize\n",
    "latent_mask = np.fromfile('qnn_calibration/vector_estimator/latent_mask.raw', dtype=np.float32)\n",
    "latent_mask = latent_mask.reshape(1, 1, 256).transpose(0, 2, 1)\n",
    "latent_mask_q = quantize(latent_mask, scale=0.0039215688593686, offset=0)\n",
    "latent_mask_q.tofile('./inputs/latent_mask.raw')\n",
    "print(f'latent_mask: {latent_mask_q.shape}, {os.path.getsize(\"./inputs/latent_mask.raw\")} bytes')\n",
    "\n",
    "# 5. text_mask - already prepared above (same quantization params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. current_step and total_step (for diffusion loop step 0 of 10)\n",
    "current_step = np.array([0], dtype=np.float32)\n",
    "current_step_q = quantize(current_step, scale=0.0000003921568634, offset=0)\n",
    "current_step_q.tofile('./inputs/current_step.raw')\n",
    "\n",
    "total_step = np.array([10], dtype=np.float32)\n",
    "total_step_q = quantize(total_step, scale=0.0196078438311815, offset=0)\n",
    "total_step_q.tofile('./inputs/total_step.raw')\n",
    "\n",
    "print(f'current_step: {current_step_q}, {os.path.getsize(\"./inputs/current_step.raw\")} bytes')\n",
    "print(f'total_step: {total_step_q}, {os.path.getsize(\"./inputs/total_step.raw\")} bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "```bash\n",
    "# Input list (vec_est_input.txt):\n",
    "# ./noisy_latent.raw ./text_emb.raw ./style_ttl.raw ./latent_mask.raw ./text_mask.raw ./current_step.raw ./total_step.raw\n",
    "\n",
    "qnn-net-run --model ./libvector_estimator_htp.so \\\n",
    "            --backend libQnnHtp.so \\\n",
    "            --input_list vec_est_input.txt \\\n",
    "            --output_dir vec_output \\\n",
    "            --use_native_input_files\n",
    "```\n",
    "\n",
    "## Output\n",
    "| Output | QNN Shape | dtype | Size |\n",
    "|--------|-----------|-------|------|\n",
    "| denoised_latent | (1, 256, 144) | FLOAT32 | 147,456 bytes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 36864 elements, range [-3.1223, 3.0408]\n",
      "Calib:  36864 elements, range [-3.5043, 3.4468]\n",
      "\n",
      "Cosine similarity (transposed): 0.1880\n"
     ]
    }
   ],
   "source": [
    "# Verify vector estimator output against vocoder calibration input\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a.flatten(), b.flatten()) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "output = np.fromfile('./board_output/denoised_latent.raw', dtype=np.float32)\n",
    "calib = np.fromfile('qnn_calibration/vocoder/latent.raw', dtype=np.float32)\n",
    "\n",
    "print(f'Output: {output.size} elements, range [{output.min():.4f}, {output.max():.4f}]')\n",
    "print(f'Calib:  {calib.size} elements, range [{calib.min():.4f}, {calib.max():.4f}]')\n",
    "\n",
    "# Output is (1, 256, 144), calib is (1, 144, 256) - need transpose\n",
    "output_t = output.reshape(1, 256, 144).transpose(0, 2, 1).flatten()\n",
    "print(f'\\nCosine similarity (transposed): {cosine_similarity(output_t, calib):.4f}')\n",
    "# Note: Low similarity (~0.19) expected with only 1 diffusion step instead of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Vocoder Input Preparation\n\nPrepares inputs for `libvocoder_htp.so` on QCS6490.\n\n| Input | ONNX Shape | QNN Shape | dtype | Quantization |\n|-------|------------|-----------|-------|--------------|\n| latent | (1, 144, 256) | (1, 256, 144) | UINT8 | scale=0.02726, offset=-129 |\n\n**Note:** Vector estimator outputs in QNN format `(1, 256, 144)` - no transpose needed.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 1. latent - from vector estimator output, no transpose needed (already QNN format)\nlatent = np.fromfile('./board_output/denoised_latent.raw', dtype=np.float32)\nlatent_q = quantize(latent, scale=0.0272593162953854, offset=-129)\nlatent_q.tofile('./inputs/latent.raw')\nprint(f'latent: {latent_q.size} elements, {os.path.getsize(\"./inputs/latent.raw\")} bytes')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Inference\n```bash\n# Input list (vocoder_input.txt):\n# ./latent.raw\n\nqnn-net-run --model ./libvocoder_htp.so \\\n            --backend libQnnHtp.so \\\n            --input_list vocoder_input.txt \\\n            --output_dir vocoder_output \\\n            --use_native_input_files\n```\n\n## Output\n| Output | QNN Shape | dtype | Size |\n|--------|-----------|-------|------|\n| wav_tts | (1, 786432) | FLOAT32 | 3,145,728 bytes |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Convert vocoder output to WAV\nfrom scipy.io import wavfile\n\nwav_data = np.fromfile('./board_output/wav_tts.raw', dtype=np.float32)\nprint(f'Audio: {wav_data.size} samples ({wav_data.size/44100:.2f} sec at 44.1kHz)')\nprint(f'Range: [{wav_data.min():.4f}, {wav_data.max():.4f}]')\n\nwavfile.write('./board_output/output.wav', 44100, wav_data)\nprint('Saved: board_output/output.wav')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}